\documentclass{beamer}
%Information to be included in the title page:
\title{1. Fundamentals}
\subtitle{GHV Chapters 4-5}
\institute{}
\date{DATA 335 -- Univerrsity of Calgary -- Winter 2025}

\DeclareMathOperator{\stderr}{se}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\CDF}{cdf}
\DeclareMathOperator{\PPF}{ppf}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Statistical models and statistical inference}


\begin{itemize}
    \item A \emph{statistical model} is a probability distribution.
    \item A statistical model is characterized by unknown and often unknowable numbers called \emph{parameters}.
    They are our quantities of interest.
    \item Statistical models facilitate \emph{statistical inference} --
    procedures for turning data into parameters estimates,
    avatars for their uncertainty.
    \begin{itemize}
        \item Frequentist inference: point estimation, standard errors, confidence intervals, hypothesis tests
        \item Bayesian inference: posterior distribution
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimators for mean and variance}
\begin{itemize}
\item Let $x_0$, $\ldots$, $x_{n - 1}$ be a
\emph{random sample}\footnote{independent and identically distributed}
from the a model (distribution) $F$ with mean $\mu$ and variance $\sigma^2$.

\item The \emph{sample mean}
$$
\bar{x} = \frac{x_0+\cdots+x_{n-1}}n
$$
estimates $\mu$.

\item The \emph{sample variance}
$$
s^2 = \frac1{n - 1}\sum_{i < n}(x_i - \bar{x})^2
$$
estimates $\sigma^2$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimators have distributions}

\begin{itemize}
    \item Since the $x_i$ are random variables, the estimators $\bar{x}$ and $s^2$ are computed computed from them are, too.
    \item In particular, they have distributions.
    \item Distributions of random variables computed from random samples from other distributions are called \emph{sampling distributions}.
    \item \textbf{(Demo)} Visualizing sampling distributions
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Standard error}
    
    \begin{itemize}
        \item The \emph{standard error} of a random variable $x$, denoted $\stderr(x)$, is the standard deviation of its distribution.
        \item $\stderr(x)$ is the fundamental numerical distillation of the uncertainty in $x$.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{The sampling distribution of the mean}

    \begin{itemize}
        \item If $x_0$, $\ldots$, $x_{n - 1}$ is a random sample drawn from
        a distribution with mean $\mu$ and standard deviation $\sigma$, then
        \[
            \stderr(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
        \]
        \item By the \emph{Central Limit Theorem}, the distribution of $\bar{x}$
        is approximately\footnote{the larger the sample size, the better the approximation} normal, with mean $\mu$ and standard deviation $\stderr(\bar{x})$.

        \item Said differently,
        \[
        z = \frac{\bar{x}-\mu}{\stderr(\bar{x})}\longrightarrow N(0, 1)
        \]
        as $n\to\infty$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Normal approximation to the binomial proportion}

    \begin{itemize}
        \item When $y\sim \Bin(n, p)$, we estimate the binomial proportion $p$ by
        \[
        \hat{p} = \frac{y}{n}.
        \]

        \item $\Bin(n, p)$-RVs are \emph{sums} of $\Ber(p)$-RVs, making $\hat{p}$
        the \emph{average} of $\Ber(p)$-RVs. Thus, $\hat{p}$ is a \emph{sample mean}.
        
        \item Since $\Ber(p)$ has standard deviation $\sqrt{p(1-p)}$,
        \[
        \stderr(\hat{p}) = \sqrt{\frac{p(1-p)}{n}}
        \]
        and, by the central limit theorem,
        \[
        \hat{p}\sim N\left(p, \frac{p(1-p)}{n}\right)
        \qquad\text{(approx.)}.
        \]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Normal approximation to the binomial distribution}
    
    \begin{itemize}
        \item Since $y=n\hat{p}$, we have
        \[
        \Bin(n, p) = \text{distribution of $y$}\approx
        N\left(np, np(1-p)\right).
        \]

        \item \textbf{(Demo)} Normal approximation to the binomial distribution
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cumulative distribution and percent point functions}

    \begin{itemize}
        \item The \emph{(cumulative) distribution function} of a random variable $x$ is defined by
        \[
            \CDF_x(u) = \mathbb{P}[x\leq u].
        \]

        \item Its inverse function is called the \emph{percent point function}.
        \[
        \PPF_x(v) = u\Longleftrightarrow \mathbb{P}[x\leq u] = v 
        \]
        Also known as the \emph{quantile function} or \emph{inverse (cumulative) distribution function}.
    \end{itemize}    
\end{frame}

\begin{frame}
    \frametitle{Confidence intervals for sample means}

    \begin{itemize}
        \item Define
        \[
        z_{\alpha/2} = \PPF_{N(0, 1)}(1 - \alpha/2).
        \]

        \item If $n$ is sufficiently large, then
        \[
        \frac{\bar{x}-\mu}{\stderr(\bar{x})}\sim N(0,1)\qquad\text{(approx.)}
        \]
        by the central limit theorem, implying
        \begin{align*}
            1 - \alpha &\approx \mathbb{P}\left[\left|\frac{\bar{x} - \mu}{\stderr(\bar{x})}\right| < z_{\alpha/2}\right]\\
        &= \mathbb{P}[\bar{x} - z_{\alpha/2}\stderr(\bar{x}) < \mu < \bar{x} + z_{\alpha/2}\stderr(\bar{x})].
        \end{align*}

        \item The interval with endpoints $\bar{x} \pm z_{\alpha/2}\stderr(\bar{x})$ is called the
        \emph{$100(1-\alpha)\%$-confidence interval} for $\mu$ associated to $\bar{x}$.

        \item \textbf{(DEMO)} Confidence intervals for sample means
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Estimating the standard error}

    \begin{itemize}
        \item Since $\sigma$ is typically unknown, so is
        \[
            \stderr(\bar{x})=\frac{\sigma}{\sqrt{n}}.
        \]
        \item To estimate it, plug in the sample standard deviation
        for $\sigma$:
        \[
        \widehat{\stderr}(\bar{x}) = \frac{s}{\sqrt{n}}.
        \]
        \item If $n$ is sufficiently large, you can use this estimate to
        constructing confidence intervals as above:
        \[
        \text{$100(1-\alpha)$\%-CI} =
        [\bar{x} - z_{\alpha/2}\widehat{\stderr}(\bar{x}),\;
        \bar{x} + z_{\alpha/2}\widehat{\stderr}(\bar{x})]
        \]
        \textbf{If $n$ is small, you can't!}

        \item \textbf{(DEMO)} Bad coverage for small $n$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Confidence intervals from the $t$-distribution}

    \begin{itemize}
        \item \textbf{If the common distribution of the $x_i$ is normal}, then
        \[
        \frac{\bar{x}-\mu}{\widehat{\stderr}(\bar{x})}\sim t_{n-1}
        \]
        ($t$-distribution with $n-1$ degrees of freedom).

        \item \textbf{(DEMO)} Simulating $t_{n-1}$
        
        \item We can use the percent-point function of $t_{n-1}$ to
        construct confidence intervals for $\bar{x}$. Set
        \[
        t_{n-1,\alpha/2} = \PPF_{t_{n-1}}(1 - \alpha/2).
        \]
        and define
        \[
        \text{$100(1-\alpha)$\%-CI} = 
        [\bar{x} - t_{n-1,\alpha/2}\widehat{\stderr}(\bar{x}),\;
        \bar{x} + t_{n-1,\alpha/2}\widehat{\stderr}(\bar{x})].
        \]

        \item This is valid for small $n$!
        For large $n$, we have $t_{n-1}\approx N(0, 1)$ and $z_{\alpha/2}\approx t_{n-1, \alpha/2}$.
    \end{itemize}
    
\end{frame}
\end{document}