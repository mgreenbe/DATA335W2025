{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 335 &mdash; Winter 2025 &mdash; Assignment 1\n",
    "\n",
    "#### Due: 2025.02.03 at 23:59\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- Save a copy of this file, changing its name to include your name and student ID as indicated.\n",
    "\n",
    "- Solve the problems, adding code and markdown cells as needed.\n",
    "\n",
    "- Your code should be runnable! Before submitting your work, *Restart* your environment and *Run All* to verify that everything works.\n",
    "\n",
    "- Please submit a `.pdf` export of your notebook (with the same name stem) to the D2L dropbox in addition to the `.ipynb` file. If you're using VS Code, click the three dots (button bar, top right) and select *Export*.\n",
    "\n",
    "#### Grading\n",
    "- All problems have equal weight.\n",
    "\n",
    "- Half-credit will be awarded for substantial progress towards a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The advantage of cross-validation\n",
    "\n",
    "In this problem, we'll identify an advantage of the $5$-fold cross-validation estimate of predictive error over a simple average of test error quantities over 5 random 80%/20% train/test splits.\n",
    "\n",
    "**To do:** Write two functions `f` and `g`.\n",
    "They should each take, as input,\n",
    "- a feature matrix `X` with `n` rows and a target vector `y` of length `n`;\n",
    "- an integer `n_repeats` (default value `1000`);\n",
    "- an integer `n_splits` (default value `5`);\n",
    "- a random seed (default value `None`)\n",
    "\n",
    "and produces, as output,\n",
    "\n",
    "- a matrix of shape `(n_repeats, n_splits)`.\n",
    "\n",
    "Each row of the ouput of `f` should consist of the `n_splits` test error quantities for a linear regression model fit to/predicted on the `n_splits` training/testing splits of `X` obtained through the cross-validation procedure. (Use `sklearn.model_selection.KFold`.)\n",
    "\n",
    "Each row of the ouput of `g` should consist of the `n_splits` test error quantities for a linear regression model fit to/predicted on `n_splits` independent training/testing splits of `X`. (Use `sklearn.model_selection.train_test_split`.)\n",
    "\n",
    "Applications of `f` with the same inputs should produce the same outputs. Randomizaton of splits should be determined by the specified seed. Same for `g`.\n",
    "\n",
    "The means of the rows of the outputs of `f` and of `g` are estimates of the expected predictive error of a linear regression model fit to a random subset of the data of size `0.8*n`.\n",
    "\n",
    "**To do:**\n",
    "\n",
    "- Run your functions `f` on `g` on the `auto_preprocessed` dataset.\n",
    "\n",
    "- Compute and compare the means and standard deviations of these row-means, and plot their histograms. What do the results suggest about the cross-validation splits versus random train/test splits? Can you explain your observations?\n",
    "\n",
    "- How large do you need to set `n_splits` in `g` to match the efficiency (i.e., the standard deviation) of the 5-fold cross-validation estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, y, *, n_repeats=1000, n_splits=5, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    kfold_mses = np.zeros((n_repeats, 5))\n",
    "\n",
    "    # Your stuff here.\n",
    "\n",
    "    return kfold_mses\n",
    "\n",
    "\n",
    "def g(X, y, *, n_repeats=1000, n_splits=5, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_test_split_mses = np.zeros((n_repeats, 5))\n",
    "\n",
    "    # Your stuff here.\n",
    "\n",
    "    return train_test_split_mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/auto_preprocessed.csv\")\n",
    "y = X.pop(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bigger models: Worth it?\n",
    "\n",
    "**To do:** Produce two feature-engineered versions, `X0` and `X1`, of the feature matrix `X` for the `data/auto_preprocessed.csv` dataset.\n",
    "\n",
    "`X0` should include:\n",
    "- all features in `X`, and\n",
    "- the squares of all the non-binary features of `X`. (Why exclude the squares of the binary features?) The binary features are the origin-indicators, `is_european` and `is_japanese`.\n",
    "\n",
    "`X1` should include:\n",
    "- `horsepower`, `weight`, `acceleration`, `year`, `is_european`, `is_japanese`; and\n",
    "- `horsepower**2`, `weight**2`, `acceleration**2`, `year**2`\n",
    "\n",
    "(`X1` removes all features involving `cylinders` or `displacement` from `X0`.)\n",
    "\n",
    "**To do:** Use 5-fold cross-validation, repeated 1000 times, to estimate the predictive errors of linear regression models fit to `(X0, y)` and `(X1, y)`. Which is better?\n",
    "\n",
    "**To do:** Use an *F*-test to compare the two models. Precisely state the null-hypothesis of the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \"Forward\" variable selection \n",
    "\n",
    "We continue working with the `data/auto_preprocessed.csv` dataset\n",
    "\n",
    "Let `X0` the the submatrix of the feature matrix `X` containing only the binary origin columns `is_european` and `is_japanese`.\n",
    "\n",
    "**(a)** Which numerical (non-binary) feature, when added to `X0`, yields the largest decrease in predictive error, as estimated by 5-fold cross-validation, repeated 100 times? \n",
    "\n",
    "**(b)** Let `X1` denote the feature matrix obtained by adding the feature identified in (a). Which numerical (non-binary) feature, when added to `X1`, yields the largest decrease in predictive error, as estimated by 5-fold cross-validation, repeated 100 times?\n",
    "\n",
    "...\n",
    "\n",
    "**(f)** Let `X5` denote the feature matrix obtained by adding the feature identified in (e). Which numerical (non-binary) feature, when added to `X5`, yields the largest decrease in predictive error, as estimated by 5-fold cross-validation, repeated 100 times?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/auto_preprocessed.csv\")\n",
    "y = X.pop(\"mpg\")\n",
    "\n",
    "X0 = X[[\"is_european\", \"is_japanese\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. \"Backward\" variable selection \n",
    "\n",
    "We continue working with the `data/auto_preprocessed.csv` dataset\n",
    "\n",
    "**(a)** Which numerical (non-binary) feature, when removed from `X`, yields the smallest increase in predictive error, as estimated by 5-fold cross-validation, repeated 100 times? \n",
    "\n",
    "**(b)** Let `X1` denote the feature matrix obtained by removing the feature identified in (a). Which numerical (non-binary) feature, when removed from `X1`, yields the smallest increase in predictive error, as estimated by 5-fold cross-validation, repeated 100 times?\n",
    "\n",
    "...\n",
    "\n",
    "**(f)** Let `X5` denote the feature matrix obtained by removing the feature identified in (e). Which numerical (non-binary) feature, when removed from `X5`, yields the smallest increase in predictive error, as estimated by 5-fold cross-validation, repeated 100 times?\n",
    "\n",
    "**(g)** Both this exercise and exercise 3 rank features by importance. How do these rankings compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/auto_preprocessed.csv\")\n",
    "y = X.pop(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lab 3, Exercise 2\n",
    "\n",
    "Here's a fake dataset describing the relationship between cholesterol level (a heart disease risk factor), age (ordinal, four categories, ages 10-30, 30-50, 50-70, and 70-90), and weekly hours of exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    n = 100\n",
    "    np.random.seed(0)\n",
    "    age = np.random.choice([0, 1, 2, 3], size=n)\n",
    "    exercise = 2 * age + 3 * np.random.normal(size=n) + 6\n",
    "    colesterol = 200 + 30 * age - 5 * exercise + 10 * np.random.normal(size=n)\n",
    "    df = pd.DataFrame({\"age\": age, \"exercise\": exercise, \"colesterol\": colesterol})\n",
    "    return df\n",
    "\n",
    "\n",
    "data = make_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the (simple) linear regression of `cholesterol` on `exercise`, overlaid on a scatterplot of the data. Do the results surprise you?\n",
    "\n",
    "- Fit a multivariate linear regression of `cholesterol` onto `exercise` and `age`. Plot the regression lines corresponding to each of the age groups, overlaid with a scatterplot of the data. Use a different color for each age group. Comment.\n",
    "\n",
    "This exercise demonstrates a phenomenon known as *Simpson's Paradox*. The inspiration for this exercise comes from &sect;1.2 of **Causal Inference in Statistics** by Pearl, Gylmour, and Jewel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lab 4, Exercise 1\n",
    "\n",
    "In this exercise, we use data about gas mileage, horsepower, and other information for 392 vehicles. See [here](https://islp.readthedocs.io/en/latest/datasets/Auto.html) for details. Use the file `data/auto.csv`.\n",
    "\n",
    "##### a)\n",
    "Without looking at the data, guess the sign of the slope of the regression of `mpg` onto `acceleration` and `mpg` onto `year`. Then fit two simple linear regression models and compare the regression slopes with your guesses. Discuss.\n",
    "\n",
    "##### b)\n",
    "Compute the group means `mpg` grouped by `origin`. Fit a regression of `mpg` onto the categorical predictor `origin` *without intercept* term. What do you observe about the coefficient estimates. Can you explain this? Now fit a regression of `mpg` onto `origin` *with intercept*. Show how the coefficients of this regression fit can be expressed in terms of the intercept-free regression coefficients, and vice-versa.\n",
    "\n",
    "##### c)\n",
    "Let `X` be the matrix of dummy variables associated to `origin`:\n",
    "```\n",
    "X = pd.get_dummies(autos[\"origin\"])\n",
    "```\n",
    "Find a number `a` and a vector `b` of shape `(3,)` such that `b.sum() == 0` and such that `a + X @ b` coincides with the predictions from the models fit in (b).\n",
    "\n",
    "##### d)\n",
    "Let `X` be as in (c) and let `n` be the vector of row counts associated to the three origins. Find a number `a` and a vector `b` of shape `(3,)` such that `np.sum(n*b) == 0` and such that `a + X @ b` coincides with the predictions from the models fit in (b). Observe that `a` equals the overall mean `auto[\"mpg\"].mean()`. The entries of `b` are \"treatments effects\" associated to the origins.\n",
    "\n",
    "##### e)\n",
    "Fit the linear regression of all the quantitative features (i.e., all the features except for `origin` and `name`) and note which non-intercept coefficient estimates have *p*-values < 0.05. Repeat with all the quantitative features except for `weight`. Compare and discuss.\n",
    "\n",
    "##### f)\n",
    "Perform an *F*-test the null hypothesis that means of the `mpg` variable are equal across all three origins after adjusting for all the continuous covariates.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
