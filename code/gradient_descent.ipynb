{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How models get fit\n",
    "\n",
    "##### 2025.04.07-2025.04.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least (mean) squares\n",
    "\n",
    "Given a matrix $X\\in\\mathbb{R}^{n\\times p}$ and a vector $y\\in\\mathbb{R}^n$,\n",
    "find the vector $\\beta\\in\\mathbb{R}^p$ that minimizes\n",
    "$$\n",
    "\\operatorname{MSE}=\\text{mean squared error}=\\frac1n\\|y - X\\beta\\|^2.\n",
    "$$\n",
    "\n",
    "If $X$ has rank $p$ (a linear algebraic nondegeneracy condition when $p\\leq n$), then this optimization problem has an unique solution. Moreover, this solution has a closed-form solution:\n",
    "$$\n",
    "\\beta = (X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 442, p = 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n",
       "       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n",
       "        751.27369956,   67.62669218])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "size, p = X.shape\n",
    "print(f\"n = {size}, p = {p}\")\n",
    "assert len(y) == size\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our $\\beta$ agrees with Scikit Learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)  # type: ignore\n",
    "assert np.allclose(beta, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple closed form expression for the $\\beta$ is unique to linear regression.\n",
    "\n",
    "There is no such for logistic, Poisson, or negative binomial regression. Nor for random effects models beyond the simplest variants.\n",
    "\n",
    "We rely on iterative algorithms that produce convengent sequences of approximations.\n",
    "\n",
    "The simplest algorithm for minimizing a function is gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients (derivatives)\n",
    "\n",
    "- We're given a real-valued function $f$ on vectors $\\beta\\in\\mathbb{R}^p$ that we want to minimize it.\n",
    "\n",
    "- Calculus furnishes us with a distinguished function $\\nabla f$ with values in $\\mathbb{R}^p$ called the ***gradient of $f$***.\n",
    "\n",
    "- $\\nabla f(\\beta)$ has the ***descent property***: If $\\nabla f(\\beta)\\neq 0$, then \n",
    "  $$\n",
    "  f\\big(\\beta - h\\nabla f(\\beta)\\big) < f(\\beta)\n",
    "  $$\n",
    "  for sufficiently small, positive $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent algorithm\n",
    "\n",
    "##### Input\n",
    "- $f$, the function we want to minimize\n",
    "- $h$, a step size\n",
    "- $\\beta$, randomly initialized vector\n",
    "\n",
    "##### Output\n",
    "- Hopefully, an approximate minimizer $\\beta'$ of $f$:\n",
    "$$\n",
    "f(\\beta')\\approx \\min_\\beta f(\\beta)\n",
    "$$\n",
    "\n",
    "##### Algorithm\n",
    "1. Set $\\beta'=\\beta - h\\nabla f(\\beta)$.\n",
    "\n",
    "2. If $f(\\beta')\\approx f(\\beta)$, return $\\beta'$. Otherwise, return to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX: NumPy + gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "size, p = X.shape\n",
    "\n",
    "\n",
    "def mse(beta):\n",
    "    return jnp.mean((y - X @ beta) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse(beta) = 29065.8125\n",
      "grad(mse)(beta) = [-1.3722459  -0.31320122 -4.2902155  -3.227192   -1.5454077  -1.2685142\n",
      "  2.8900678  -3.1469736  -4.1378827  -2.7954698 ]\n",
      "\n",
      "v = 29065.8125\n",
      "g = [-1.3722459  -0.31320122 -4.2902155  -3.227192   -1.5454077  -1.2685142\n",
      "  2.8900678  -3.1469736  -4.1378827  -2.7954698 ]\n"
     ]
    }
   ],
   "source": [
    "beta = jnp.array(np.random.uniform(size=p))\n",
    "print(f\"mse(beta) = {mse(beta)}\")\n",
    "print(f\"grad(mse)(beta) = {grad(mse)(beta)}\")\n",
    "\n",
    "v, g = value_and_grad(mse)(beta)\n",
    "print(f\"\\nv = {v}\")\n",
    "print(f\"g = {g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: mse(beta) = 29066.29296875\n",
      "i = 500: mse(beta) = 26004.755859375\n",
      "i = 1000: mse(beta) = 26004.3046875\n",
      "i = 1500: mse(beta) = 26004.294921875\n",
      "i = 2000: mse(beta) = 26004.296875\n",
      "i = 2500: mse(beta) = 26004.29296875\n",
      "\n",
      "beta = [ -10.009933 -239.8156    519.846     324.38474  -792.1676    476.73227\n",
      "  101.04023   177.06308   751.2704     67.626884]\n"
     ]
    }
   ],
   "source": [
    "h = 100.0\n",
    "max_iter = 3000\n",
    "beta = jnp.array(np.random.uniform(size=p))\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(mse)(beta)\n",
    "    if i % 500 == 0:\n",
    "        print(f\"i = {i}: mse(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta = {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn model coefficients (copied and pasted from above):\n",
    "```text\n",
    "array([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n",
    "       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n",
    "        751.27369956,   67.62669218])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "- Instead of minimizing MSE, we minimize ***log-loss***:\n",
    "$$\n",
    "L(\\beta) = \\frac1n\\left(\\sum_{i<n}\\big(-y_i\\log p_i - (1 - y_i)\\log(1 - p_i)\\big)\\right),\n",
    "\\qquad p_i = \\frac1{1 + e^{-X\\beta}}\n",
    "$$\n",
    "\n",
    "Using the identities,\n",
    "$$\n",
    "\\log p_i = -\\log(1 + e^{-X\\beta})\\qquad\\text{and}\\qquad \\log(1 - p_i) = -\\log(1 + e^{X\\beta})\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "L(\\beta) = \\frac1n\\left(\\sum_{i<n}\\big(y_i\\log (1 + e^{-X\\beta}) + (1 - y_i)\\log(1 + e^{X\\beta})\\big)\\right).\n",
    "$$\n",
    "\n",
    "##### Remark\n",
    "\n",
    "The function $f(x)=-\\log(1 + e^{-x})$ is called the ***log-sigmoid function***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.coef_.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n",
      "\n",
      "loss = 0.3745998184276785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "size = 200\n",
    "p = 10\n",
    "rng = np.random.default_rng(42)\n",
    "X = rng.normal(size=(size, p))\n",
    "beta_true = rng.normal(size=p)\n",
    "probs = 1 / (1 + np.exp(-X @ beta_true))\n",
    "y = (rng.random(size=size) < probs).astype(int)\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=np.inf, max_iter=1000, fit_intercept=False)\n",
    "model.fit(X, y)  # type: ignore\n",
    "loss = log_loss(y, model.predict_proba(X)[:, 1])\n",
    "print(f\"model.coef_.round(2) = {model.coef_.squeeze().round(2)}\")\n",
    "print(f\"\\nloss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: my_log_loss(beta) = 1.0123999118804932\n",
      "i = 10: my_log_loss(beta) = 0.37464815378189087\n",
      "i = 20: my_log_loss(beta) = 0.3745999038219452\n",
      "i = 30: my_log_loss(beta) = 0.37459975481033325\n",
      "i = 40: my_log_loss(beta) = 0.37459975481033325\n",
      "\n",
      "beta.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n"
     ]
    }
   ],
   "source": [
    "def my_log_loss(beta):\n",
    "    return jnp.mean(y * jnp.log(1 + jnp.exp(-X @ beta)) + (1 - y) * jnp.log(1 + jnp.exp(X @ beta)))  # type: ignore\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=p))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(my_log_loss)(beta)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i = {i}: my_log_loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-sigmoid function is built into JAX. We can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: my_log_loss(beta) = 1.6029266119003296\n",
      "i = 10: my_log_loss(beta) = 0.37462809681892395\n",
      "i = 20: my_log_loss(beta) = 0.37459999322891235\n",
      "i = 30: my_log_loss(beta) = 0.37459975481033325\n",
      "i = 40: my_log_loss(beta) = 0.37459975481033325\n",
      "\n",
      "beta.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n"
     ]
    }
   ],
   "source": [
    "from jax.nn import log_sigmoid\n",
    "\n",
    "\n",
    "def my_log_loss(beta):\n",
    "    return -jnp.mean(y * log_sigmoid(X @ beta) + (1 - y) * log_sigmoid(-X @ beta))\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=p))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(my_log_loss)(beta)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i = {i}: my_log_loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### Softmax regression\n",
    "\n",
    "$$\n",
    "\\frac{p(v_j\\mid \\beta)}{p(v_0\\mid \\beta)} = e^{X\\beta_j},\\qquad 0 < j < n\\tag{$\\dag$}\n",
    "$$\n",
    "\n",
    "Solving\n",
    "$$\n",
    "\\frac1{p(v_0\\mid\\beta)} =\\sum_k \\frac{p(v_k\\mid \\beta)}{p(v_0\\mid\\beta)} = \\sum_k e^{X\\beta_k}\n",
    "$$\n",
    "for $p(v_0\\mid\\beta)$, we get\n",
    "$$\n",
    "p(v_0\\mid\\beta) = \\frac1{\\sum_j e^{X\\beta_j}}.\n",
    "$$\n",
    "\n",
    "Letting $\\beta_0=0$ (the zero vector), we have\n",
    "$$\n",
    "p(v_j\\mid\\beta)=\\frac{e^{X\\beta_j}}{\\sum_ke^{X\\beta_k}}\n",
    "$$\n",
    "for all $j$.\n",
    "\n",
    "Define the ***softmax function***:\n",
    "$$\n",
    "\\operatorname{softmax}:\\mathbb{R}^c\\longrightarrow \\mathbb{R}^c,\\qquad\n",
    "\\operatorname{softmax}(\\mu) = \\left(\n",
    "    \\frac{e^{\\mu_0}}{\\sum_k e^{\\mu_k}},\\ldots, \\frac{e^{\\mu_{c-1}}}{\\sum_k e^{\\mu_k}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "log-likelihood function:\n",
    "$$\n",
    "\\log p(y_i\\mid x_i, \\beta) = \\operatorname{sum}(y_i\\odot\\log\\operatorname{softmax}(x_i\\beta))\n",
    "$$\n",
    "\n",
    "Batched version:\n",
    "$$\n",
    "\\log p(Y\\mid X,\\beta) = \\operatorname{sum}(Y\\odot\\log\\operatorname{softmax}(X\\beta,\\, \\text{\\texttt{axis=1}}),\\, \\text{\\texttt{axis=1}})\n",
    "$$\n",
    "\n",
    "Conveniently, the log-softmax function is built into JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84 -0.16  1.12 -1.79]\n",
      " [ 0.3  -0.13 -1.56  1.39]\n",
      " [-0.66  0.03  1.44 -0.81]\n",
      " [-0.54  0.22  0.82 -0.5 ]\n",
      " [-0.1   1.41  0.41 -1.72]\n",
      " [-1.16  1.3   0.45 -0.59]\n",
      " [-0.02  0.4   0.88 -1.25]\n",
      " [-0.74  0.05 -0.02  0.72]\n",
      " [-1.54  0.34  0.73  0.47]\n",
      " [ 0.27  0.36 -0.61 -0.02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "size = 300\n",
    "p = 10\n",
    "c = 4\n",
    "rng = np.random.default_rng(42)\n",
    "X = rng.normal(size=(size, p))\n",
    "beta = rng.normal(size=(p, c))\n",
    "z = np.exp(X @ beta)\n",
    "probs = z / z.sum(axis=1, keepdims=True)\n",
    "y = np.argmax(rng.uniform(size=(size, 1)) < np.cumsum(probs, axis=1), axis=1)\n",
    "\n",
    "model = LogisticRegression(C=np.inf, max_iter=1000, fit_intercept=False, tol=1e-8)\n",
    "model.fit(X, y)  # type: ignore\n",
    "accuracy_score(y, model.predict(X))\n",
    "\n",
    "print(model.coef_.squeeze().round(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: loss(beta) = 3.732680559158325\n",
      "i = 2: loss(beta) = 0.5561644434928894\n",
      "i = 4: loss(beta) = 0.5440211892127991\n",
      "i = 6: loss(beta) = 0.542653501033783\n",
      "i = 8: loss(beta) = 0.5423337817192078\n",
      "i = 10: loss(beta) = 0.5422449111938477\n",
      "i = 12: loss(beta) = 0.5422179102897644\n",
      "i = 14: loss(beta) = 0.5422090291976929\n",
      "i = 16: loss(beta) = 0.542205810546875\n",
      "i = 18: loss(beta) = 0.5422044992446899\n",
      "i = 20: loss(beta) = 0.5422039031982422\n",
      "i = 22: loss(beta) = 0.5422036051750183\n",
      "i = 24: loss(beta) = 0.5422035455703735\n",
      "i = 26: loss(beta) = 0.542203426361084\n",
      "i = 28: loss(beta) = 0.542203426361084\n",
      "\n",
      "beta.round(2) = [[ 1.13  0.13  1.42 -1.5 ]\n",
      " [ 0.3  -0.13 -1.55  1.4 ]\n",
      " [-1.27 -0.59  0.83 -1.42]\n",
      " [-1.27 -0.51  0.1  -1.22]\n",
      " [-0.4   1.11  0.11 -2.02]\n",
      " [-1.85  0.61 -0.23 -1.28]\n",
      " [-0.41  0.01  0.49 -1.64]\n",
      " [-0.59  0.2   0.14  0.87]\n",
      " [-1.62  0.26  0.65  0.39]\n",
      " [-0.34 -0.26 -1.22 -0.64]]\n"
     ]
    }
   ],
   "source": [
    "from jax.nn import log_softmax\n",
    "\n",
    "Y = np.zeros((size, c))\n",
    "np.put_along_axis(arr=Y, indices=y.reshape(-1, 1), values=1, axis=1)\n",
    "\n",
    "\n",
    "def loss(beta):\n",
    "    return -jnp.sum(Y * log_softmax(X @ beta)) / size\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 30\n",
    "beta = jnp.array(np.random.normal(size=(p, c)))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss)(beta)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74, -0.19,  0.95, -1.51],\n",
       "       [ 0.21, -0.05, -1.29,  1.13],\n",
       "       [-0.53, -0.04,  1.24, -0.67],\n",
       "       [-0.45,  0.16,  0.7 , -0.41],\n",
       "       [-0.04,  1.17,  0.28, -1.42],\n",
       "       [-0.96,  1.1 ,  0.35, -0.48],\n",
       "       [-0.02,  0.31,  0.74, -1.04],\n",
       "       [-0.63,  0.02, -0.01,  0.62],\n",
       "       [-1.3 ,  0.26,  0.6 ,  0.44],\n",
       "       [ 0.22,  0.33, -0.51, -0.03]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "model.coef_.round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "beta_skl = model.coef_.round(2).T\n",
    "loss(beta_skl)\n",
    "accuracy_score(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(X @ beta, axis=1)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time using identifiable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(n: int, p: int, c: int, rng: int | np.random.Generator | None = None):\n",
    "    if c < 2:\n",
    "        raise ValueError(\"Number of classes c must be > 1.\")\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n, p))\n",
    "    beta = np.zeros((p, c))\n",
    "    beta[:, 1:] = rng.normal(size=(p, c - 1))\n",
    "    z = np.exp(X @ beta)\n",
    "    probs = z / z.sum(axis=1, keepdims=True)\n",
    "    y = np.argmax(\n",
    "        rng.uniform(size=(n, 1)) < np.cumsum(probs, axis=1), axis=1, keepdims=True\n",
    "    )\n",
    "    Y = np.zeros((n, c))\n",
    "    np.put_along_axis(arr=Y, indices=y, values=1, axis=1)\n",
    "    return X, Y, beta[:, 1:]\n",
    "\n",
    "\n",
    "def model(beta, X):\n",
    "    p = len(beta)\n",
    "    full_beta = jnp.hstack([jnp.zeros((p, 1)), beta])\n",
    "    logits = X @ full_beta\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(beta, X, Y):\n",
    "    logits = model(beta, X)\n",
    "    return -jnp.sum(Y * log_softmax(logits)) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = 0.7099\n",
      "i = 0: loss(beta) = 3.6220591068267822\n",
      "i = 5: loss(beta) = 0.7253149151802063\n",
      "i = 10: loss(beta) = 0.714459240436554\n",
      "i = 15: loss(beta) = 0.7138931155204773\n",
      "i = 20: loss(beta) = 0.7138315439224243\n",
      "i = 25: loss(beta) = 0.7138221263885498\n",
      "i = 30: loss(beta) = 0.7138205170631409\n",
      "i = 35: loss(beta) = 0.7138200998306274\n",
      "i = 40: loss(beta) = 0.7138200998306274\n",
      "i = 45: loss(beta) = 0.7138200998306274\n",
      "training loss = 0.7099\n",
      "max error = 1.93%\n"
     ]
    }
   ],
   "source": [
    "size = 10000000\n",
    "p = 10\n",
    "c = 4\n",
    "X, Y, _beta = data(size, p, c)\n",
    "print(\n",
    "    f\"true loss = {np.mean(\n",
    "        np.argmax(X @ np.hstack([np.zeros((p, 1)), _beta]), axis=1)\n",
    "        == np.argmax(Y, axis=1)\n",
    "    ):.4f}\"\n",
    ")\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=(p, c - 1)))\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, X, Y)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "\n",
    "print(\n",
    "    f\"training loss = {np.mean(\n",
    "        np.argmax(X @ np.hstack([np.zeros((p, 1)), beta]), axis=1)\n",
    "        == np.argmax(Y, axis=1)\n",
    "    ):.4f}\"\n",
    ")\n",
    "\n",
    "assert np.all(np.sign(beta) == np.sign(_beta))\n",
    "print(\n",
    "    f\"max error = {100*np.max((beta - _beta) / (0.5 * np.abs(beta + _beta))).item():.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson regression\n",
    "\n",
    "$$\n",
    "y_i\\sim \\operatorname{Poisson}(e^{x_i\\beta})\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y_i\\mid x_i,\\beta) = \\frac{e^{y_ix_i\\beta}e^{-e^{x_i\\beta}}}{y_i!}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y_i\\mid x_i,\\beta) &= \\log\\frac{e^{y_ix_i\\beta}e^{-e^{x_i\\beta}}}{y_i!}\\\\\n",
    "&= y_ix_i\\beta - e^{x_i\\beta} - \\log y_i!\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $y_i!$ is gigantic is $y_i$ is big, and we only want its logarithm anyways, it's best to avoid computing directly. Better to use the ***log-gamma function***:\n",
    "$$\n",
    "\\log y_i! = \\log\\Gamma(y_i + 1)\n",
    "$$\n",
    "\n",
    "Negative log-likelihood:\n",
    "$$\n",
    "\\ell(\\beta) = -\\frac1n\\operatorname{sum}(Y\\odot (X\\beta) - e^{X\\beta} - \\log \\Gamma(Y+1))\n",
    "$$\n",
    "\n",
    "We can actually drop the log-gamma term since it doesn't depend on $\\beta$.\n",
    "\n",
    "$$\n",
    "\\operatorname{loss} = -\\operatorname{mean}(Y\\odot (X\\beta) - e^{X\\beta})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(n: int, p: int, rng: int | np.random.Generator | None = None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n, p))\n",
    "    beta = rng.normal(size=p)\n",
    "    lam = np.exp(X @ beta)\n",
    "    y = rng.poisson(lam).astype(float)\n",
    "    return X, y, beta\n",
    "\n",
    "\n",
    "def loss(beta, X, y):\n",
    "    log_lam = X @ beta\n",
    "    return -jnp.mean(y * log_lam - jnp.exp(log_lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = -6.6776\n",
      "i = 0: loss(beta) = -0.44397902488708496\n",
      "i = 50: loss(beta) = -6.603349685668945\n",
      "i = 100: loss(beta) = -6.676112651824951\n",
      "i = 150: loss(beta) = -6.677542686462402\n",
      "i = 200: loss(beta) = -6.677570819854736\n",
      "i = 250: loss(beta) = -6.677570343017578\n",
      "i = 300: loss(beta) = -6.677570819854736\n",
      "i = 350: loss(beta) = -6.677570819854736\n",
      "i = 400: loss(beta) = -6.677570819854736\n",
      "i = 450: loss(beta) = -6.677570819854736\n",
      "training loss = -6.6776\n"
     ]
    }
   ],
   "source": [
    "size = 1000000\n",
    "p = 3\n",
    "X, y, _beta = data(size, p)\n",
    "print(f\"true loss = {loss(_beta, X, y):.4f}\")\n",
    "\n",
    "h = 0.01\n",
    "max_iter = 500\n",
    "beta = jnp.array(np.random.normal(size=p))\n",
    "# beta = jnp.zeros(p)\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, X, y)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"training loss = {loss(beta, X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative binomial regression\n",
    "\n",
    "#### $(r, p)$-parametrization\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y\\mid n, p) &= \\binom{y + n - 1}{y}(1 - p)^yp^n\\\\\n",
    "&\\propto\\frac{\\Gamma(y + n)}{\\Gamma(n)}(1 - p)^yp^n\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y\\mid n, p) &\\propto  \\log\\Gamma(y + n) - \\log\\Gamma(n) + y\\log(1 - p) + n\\log p\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\\mu = \\mathbb{E}[y] = \\frac{n(1-p)}{p},\\qquad\n",
    "\\sigma^2=\\mathbb{V}[y] = \\frac{n(1-p)}{p^2} = \\mu + \\frac{\\mu^2}{n}\n",
    "$$\n",
    "\n",
    "Solve for $p$ in terms of $\\mu$ and $n$,\n",
    "$$\n",
    "p = \\frac{n}{\\mu + n},\n",
    "$$\n",
    "note that\n",
    "$$\n",
    "1 - p = \\frac{\\mu}{\\mu + n},\n",
    "$$\n",
    "and eliminate $p$ entirely:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y\\mid \\mu, n) &= \\binom{y + n - 1}{y}\n",
    "\\left(\\frac{\\mu}{\\mu + n}\\right)^y\n",
    "\\left(\\frac{n}{\\mu + n}\\right)^n\\\\\n",
    "&= \\frac{\\Gamma(y + n)}{y!\\Gamma(n)}\n",
    "\\left(\\frac{\\mu}{\\mu + n}\\right)^y\n",
    "\\left(\\frac{n}{\\mu + n}\\right)^n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y\\mid \\mu, n) &= \\log\\Gamma(y + n) - \\log\\Gamma(n) - \\log y!\\\\\n",
    "&\\phantom{=}{} + y\\log\\mu - (y + n)\\log(\\mu + n) + n\\log n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mean = 8.02, distribution mean = 8.00\n",
      "sample variance = 24.03, distribution variance = 24.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax.scipy.stats as jstats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "p = 1 / 3\n",
    "n = 4\n",
    "size = 10_000\n",
    "\n",
    "# negative binomial sampling with numpy.random\n",
    "# no equivalent in jax.numpy.random\n",
    "y = rng.negative_binomial(n, p, size=size)\n",
    "counter = Counter(y)\n",
    "values = np.array(list(counter.keys()))\n",
    "counts = np.array(list(counter.values()))\n",
    "\n",
    "print(f\"sample mean = {y.mean():.2f}, distribution mean = {n*(1-p)/p:.2f}\")\n",
    "print(f\"sample variance = {y.var():.2f}, distribution variance = {n*(1-p)/p**2:.2f}\")\n",
    "\n",
    "# compute pmf-values using jax.scipy.stats.nbinom\n",
    "masses = jstats.nbinom.pmf(values, n, p)\n",
    "\n",
    "plt.vlines(values, 0, counts / size, colors=\"C0\", label=\"observed\")\n",
    "plt.vlines(values + 0.5, 0, masses, colors=\"C1\", label=\"predicted\")\n",
    "plt.title(f\"{size} draws from $\\\\operatorname{{NB}}(n={n}, p={p:.2f})$\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlim(-2, 32)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(beta, y):\n",
    "    n, p = beta\n",
    "    return -jstats.nbinom.logpmf(y, n, p).mean()\n",
    "\n",
    "\n",
    "def data(size: int, n: float | None = None, p: float | None = None, rng=None):\n",
    "    if n is None or p is None:\n",
    "        raise ValueError()\n",
    "    rng = np.random.default_rng(rng)\n",
    "    return rng.negative_binomial(n, p, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = 2.9209\n",
      "i = 0: loss(beta) = 6.2822699546813965, n = 1.0000, p = 0.5000\n",
      "i = 200: loss(beta) = 2.9688873291015625, n = 2.1759, p = 0.2279\n",
      "i = 400: loss(beta) = 2.944277048110962, n = 2.5085, p = 0.2372\n",
      "i = 600: loss(beta) = 2.9359540939331055, n = 2.7379, p = 0.2534\n",
      "i = 800: loss(beta) = 2.931231737136841, n = 2.9109, p = 0.2652\n",
      "i = 1000: loss(beta) = 2.9282848834991455, n = 3.0478, p = 0.2743\n",
      "i = 1200: loss(beta) = 2.92632794380188, n = 3.1593, p = 0.2815\n",
      "i = 1400: loss(beta) = 2.9249696731567383, n = 3.2522, p = 0.2874\n",
      "i = 1600: loss(beta) = 2.9239964485168457, n = 3.3309, p = 0.2923\n",
      "i = 1800: loss(beta) = 2.9232850074768066, n = 3.3982, p = 0.2965\n",
      "i = 2000: loss(beta) = 2.9227511882781982, n = 3.4565, p = 0.3000\n",
      "i = 2200: loss(beta) = 2.9223477840423584, n = 3.5072, p = 0.3031\n",
      "i = 2400: loss(beta) = 2.922037363052368, n = 3.5517, p = 0.3058\n",
      "i = 2600: loss(beta) = 2.9217963218688965, n = 3.5910, p = 0.3081\n",
      "i = 2800: loss(beta) = 2.9216060638427734, n = 3.6257, p = 0.3102\n",
      "i = 3000: loss(beta) = 2.9214558601379395, n = 3.6566, p = 0.3120\n",
      "i = 3200: loss(beta) = 2.921337127685547, n = 3.6841, p = 0.3136\n",
      "i = 3400: loss(beta) = 2.9212424755096436, n = 3.7087, p = 0.3150\n",
      "i = 3600: loss(beta) = 2.921165943145752, n = 3.7308, p = 0.3163\n",
      "i = 3800: loss(beta) = 2.9211044311523438, n = 3.7506, p = 0.3175\n",
      "i = 4000: loss(beta) = 2.9210526943206787, n = 3.7684, p = 0.3185\n",
      "i = 4200: loss(beta) = 2.921013832092285, n = 3.7845, p = 0.3194\n",
      "i = 4400: loss(beta) = 2.92098069190979, n = 3.7991, p = 0.3203\n",
      "i = 4600: loss(beta) = 2.920954704284668, n = 3.8122, p = 0.3210\n",
      "i = 4800: loss(beta) = 2.9209303855895996, n = 3.8241, p = 0.3217\n",
      "i = 5000: loss(beta) = 2.9209144115448, n = 3.8348, p = 0.3223\n",
      "i = 5200: loss(beta) = 2.9208991527557373, n = 3.8446, p = 0.3229\n",
      "i = 5400: loss(beta) = 2.9208855628967285, n = 3.8535, p = 0.3234\n",
      "i = 5600: loss(beta) = 2.9208736419677734, n = 3.8615, p = 0.3238\n",
      "i = 5800: loss(beta) = 2.9208662509918213, n = 3.8688, p = 0.3242\n",
      "i = 6000: loss(beta) = 2.920860528945923, n = 3.8755, p = 0.3246\n",
      "i = 6200: loss(beta) = 2.92085337638855, n = 3.8815, p = 0.3250\n",
      "i = 6400: loss(beta) = 2.920849561691284, n = 3.8870, p = 0.3253\n",
      "i = 6600: loss(beta) = 2.9208452701568604, n = 3.8921, p = 0.3255\n",
      "i = 6800: loss(beta) = 2.9208436012268066, n = 3.8966, p = 0.3258\n",
      "i = 7000: loss(beta) = 2.9208412170410156, n = 3.9008, p = 0.3260\n",
      "i = 7200: loss(beta) = 2.920836925506592, n = 3.9046, p = 0.3263\n",
      "i = 7400: loss(beta) = 2.920834541320801, n = 3.9081, p = 0.3264\n",
      "i = 7600: loss(beta) = 2.9208333492279053, n = 3.9113, p = 0.3266\n",
      "i = 7800: loss(beta) = 2.920833110809326, n = 3.9142, p = 0.3268\n",
      "i = 8000: loss(beta) = 2.9208312034606934, n = 3.9168, p = 0.3269\n",
      "i = 8200: loss(beta) = 2.920830488204956, n = 3.9192, p = 0.3271\n",
      "i = 8400: loss(beta) = 2.920828342437744, n = 3.9214, p = 0.3272\n",
      "i = 8600: loss(beta) = 2.9208309650421143, n = 3.9234, p = 0.3273\n",
      "i = 8800: loss(beta) = 2.920828342437744, n = 3.9253, p = 0.3274\n",
      "i = 9000: loss(beta) = 2.920830011367798, n = 3.9269, p = 0.3275\n",
      "i = 9200: loss(beta) = 2.9208264350891113, n = 3.9285, p = 0.3276\n",
      "i = 9400: loss(beta) = 2.920828104019165, n = 3.9299, p = 0.3277\n",
      "i = 9600: loss(beta) = 2.9208261966705322, n = 3.9311, p = 0.3277\n",
      "i = 9800: loss(beta) = 2.920828342437744, n = 3.9323, p = 0.3278\n",
      "training loss = 2.9208\n",
      "n = 3.9334, p = 0.3279\n"
     ]
    }
   ],
   "source": [
    "y = data(10000, n=4, p=1 / 3)\n",
    "_beta = jnp.array([4, 1 / 3])\n",
    "print(f\"true loss = {loss(_beta, y):.4f}\")\n",
    "\n",
    "beta = jnp.array([1.0, 0.5])\n",
    "h = 0.032\n",
    "max_iter = 10000\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, y)\n",
    "    if i % 200 == 0:\n",
    "        print(\n",
    "            f\"i = {i}: loss(beta) = {v}, n = {beta[0].item():.4f}, p = {beta[1].item():.4f}\"\n",
    "        )\n",
    "    beta -= h * g\n",
    "print(f\"training loss = {loss(beta, y):.4f}\")\n",
    "\n",
    "size = beta[0].item()\n",
    "p = beta[1].item()\n",
    "print(f\"n = {size:.4f}, p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom n = 3.8758, mom p = 0.3246\n"
     ]
    }
   ],
   "source": [
    "mu = y.mean()\n",
    "sigma = y.std()\n",
    "\n",
    "mom_n = mu**2 / (sigma**2 - mu)\n",
    "mom_p = mom_n / (mu + mom_n)\n",
    "print(f\"mom n = {mom_n:.4f}, mom p = {mom_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(\n",
    "    n_examples: int,\n",
    "    n_features: int,\n",
    "    n: float,\n",
    "    rng: int | np.random.Generator | None = None,\n",
    "):\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n_examples, n_features))\n",
    "    beta = rng.normal(size=n_features)\n",
    "    mu = np.exp(X @ beta)\n",
    "    p = n / (mu + n)\n",
    "    y = rng.negative_binomial(n, p)\n",
    "    return X, y, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, beta = data(300, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  3,  1,  1,  4,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0, 18,  3,\n",
       "        5,  0,  0,  0,  1,  0,  0,  7,  1, 10,  0,  1,  1,  0, 31,  0,  0,\n",
       "        0,  0, 16,  4,  0,  0,  6,  0,  4,  1,  0,  0,  4,  0,  3,  0,  9,\n",
       "        1,  2,  1,  4,  4,  9,  2,  0,  1,  4,  0,  3,  2,  3,  1, 19,  1,\n",
       "        0,  1,  2,  0,  0, 12,  0,  0,  2,  0,  0,  0,  0,  0,  3,  0,  0,\n",
       "        3,  4,  1,  0,  0,  0,  0,  3,  6,  2,  2,  0,  1,  0,  2,  9,  2,\n",
       "        0,  2,  2,  0,  0,  1,  0,  8,  1,  4,  2,  3,  0,  0,  1,  0,  2,\n",
       "        1,  4, 24,  0,  1,  2,  4,  3,  0,  0,  0,  0,  0,  1,  5,  3,  0,\n",
       "       12,  3,  3,  1,  7,  0,  0,  0, 10,  1,  0,  1,  2,  0, 23,  1,  0,\n",
       "        0,  0,  5,  1,  3,  1,  2,  0,  1, 22,  0,  1,  0,  0, 68, 11,  1,\n",
       "        1,  6,  0,  0,  0,  0,  0,  0,  1,  6,  6,  1,  2,  2,  0,  6,  0,\n",
       "        0, 10,  0,  0,  6,  0,  1,  0, 29,  7,  0,  3,  0,  2, 13,  1,  1,\n",
       "        0,  1,  2,  2,  0,  0,  1,  1,  1,  0,  3,  0,  0,  0,  6,  0,  9,\n",
       "        2,  1,  0,  0,  1,  2,  0,  0,  3, 40,  4,  5,  3,  1,  0,  1,  2,\n",
       "        3,  0,  6,  2,  6,  0,  5,  1,  3,  7,  2,  1,  6,  0,  1,  0,  4,\n",
       "        1,  1,  2,  0,  0, 13,  0,  1,  3,  0,  1,  0,  0,  0,  2,  2,  3,\n",
       "        0,  0,  3,  3,  1,  0,  4,  3,  0,  1,  0,  0,  0,  1,  0,  0,  0,\n",
       "        1,  1,  2,  0,  1,  1,  0,  0,  1,  0,  0])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
