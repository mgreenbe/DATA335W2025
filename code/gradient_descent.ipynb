{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How models get fit\n",
    "\n",
    "##### 2025.04.07-2025.04.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least (mean) squares\n",
    "\n",
    "Given a matrix $X\\in\\mathbb{R}^{n\\times p}$ and a vector $y\\in\\mathbb{R}^n$,\n",
    "find the vector $\\beta\\in\\mathbb{R}^p$ that minimizes\n",
    "$$\n",
    "\\operatorname{MSE}=\\text{mean squared error}=\\frac1n\\|y - X\\beta\\|^2.\n",
    "$$\n",
    "\n",
    "If $X$ has rank $p$ (a linear algebraic nondegeneracy condition when $p\\leq n$), then this optimization problem has an unique solution. Moreover, this solution has a closed-form solution:\n",
    "$$\n",
    "\\beta = (X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 442, p = 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n",
       "       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n",
       "        751.27369956,   67.62669218])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "size, p = X.shape\n",
    "print(f\"n = {size}, p = {p}\")\n",
    "assert len(y) == size\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our $\\beta$ agrees with Scikit Learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)  # type: ignore\n",
    "assert np.allclose(beta, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple closed form expression for the $\\beta$ is unique to linear regression.\n",
    "\n",
    "There is no such for logistic, Poisson, or negative binomial regression. Nor for random effects models beyond the simplest variants.\n",
    "\n",
    "We rely on iterative algorithms that produce convengent sequences of approximations.\n",
    "\n",
    "The simplest algorithm for minimizing a function is gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients (derivatives)\n",
    "\n",
    "- We're given a real-valued function $f$ on vectors $\\beta\\in\\mathbb{R}^p$ that we want to minimize it.\n",
    "\n",
    "- Calculus furnishes us with a distinguished function $\\nabla f$ with values in $\\mathbb{R}^p$ called the ***gradient of $f$***.\n",
    "\n",
    "- $\\nabla f(\\beta)$ has the ***descent property***: If $\\nabla f(\\beta)\\neq 0$, then \n",
    "  $$\n",
    "  f\\big(\\beta - h\\nabla f(\\beta)\\big) < f(\\beta)\n",
    "  $$\n",
    "  for sufficiently small, positive $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent algorithm\n",
    "\n",
    "##### Input\n",
    "- $f$, the function we want to minimize\n",
    "- $h$, a step size\n",
    "- $\\beta$, randomly initialized vector\n",
    "\n",
    "##### Output\n",
    "- Hopefully, an approximate minimizer $\\beta'$ of $f$:\n",
    "$$\n",
    "f(\\beta')\\approx \\min_\\beta f(\\beta)\n",
    "$$\n",
    "\n",
    "##### Algorithm\n",
    "1. Set $\\beta'=\\beta - h\\nabla f(\\beta)$.\n",
    "\n",
    "2. If $f(\\beta')\\approx f(\\beta)$, return $\\beta'$. Otherwise, return to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX: NumPy + gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "size, p = X.shape\n",
    "\n",
    "\n",
    "def mse(beta):\n",
    "    return jnp.mean((y - X @ beta) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse(beta) = 29065.8125\n",
      "grad(mse)(beta) = [-1.3722459  -0.31320122 -4.2902155  -3.227192   -1.5454077  -1.2685142\n",
      "  2.8900678  -3.1469736  -4.1378827  -2.7954698 ]\n",
      "\n",
      "v = 29065.8125\n",
      "g = [-1.3722459  -0.31320122 -4.2902155  -3.227192   -1.5454077  -1.2685142\n",
      "  2.8900678  -3.1469736  -4.1378827  -2.7954698 ]\n"
     ]
    }
   ],
   "source": [
    "beta = jnp.array(np.random.uniform(size=p))\n",
    "print(f\"mse(beta) = {mse(beta)}\")\n",
    "print(f\"grad(mse)(beta) = {grad(mse)(beta)}\")\n",
    "\n",
    "v, g = value_and_grad(mse)(beta)\n",
    "print(f\"\\nv = {v}\")\n",
    "print(f\"g = {g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: mse(beta) = 29066.29296875\n",
      "i = 500: mse(beta) = 26004.755859375\n",
      "i = 1000: mse(beta) = 26004.3046875\n",
      "i = 1500: mse(beta) = 26004.294921875\n",
      "i = 2000: mse(beta) = 26004.296875\n",
      "i = 2500: mse(beta) = 26004.29296875\n",
      "\n",
      "beta = [ -10.009933 -239.8156    519.846     324.38474  -792.1676    476.73227\n",
      "  101.04023   177.06308   751.2704     67.626884]\n"
     ]
    }
   ],
   "source": [
    "h = 100.0\n",
    "max_iter = 3000\n",
    "beta = jnp.array(np.random.uniform(size=p))\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(mse)(beta)\n",
    "    if i % 500 == 0:\n",
    "        print(f\"i = {i}: mse(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta = {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn model coefficients (copied and pasted from above):\n",
    "```text\n",
    "array([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n",
    "       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n",
    "        751.27369956,   67.62669218])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "- Instead of minimizing MSE, we minimize ***log-loss***:\n",
    "$$\n",
    "L(\\beta) = \\frac1n\\left(\\sum_{i<n}\\big(-y_i\\log p_i - (1 - y_i)\\log(1 - p_i)\\big)\\right),\n",
    "\\qquad p_i = \\frac1{1 + e^{-X\\beta}}\n",
    "$$\n",
    "\n",
    "Using the identities,\n",
    "$$\n",
    "\\log p_i = -\\log(1 + e^{-X\\beta})\\qquad\\text{and}\\qquad \\log(1 - p_i) = -\\log(1 + e^{X\\beta})\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "L(\\beta) = \\frac1n\\left(\\sum_{i<n}\\big(y_i\\log (1 + e^{-X\\beta}) + (1 - y_i)\\log(1 + e^{X\\beta})\\big)\\right).\n",
    "$$\n",
    "\n",
    "##### Remark\n",
    "\n",
    "The function $f(x)=-\\log(1 + e^{-x})$ is called the ***log-sigmoid function***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.coef_.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n",
      "\n",
      "loss = 0.3745998184276785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "size = 200\n",
    "p = 10\n",
    "rng = np.random.default_rng(42)\n",
    "X = rng.normal(size=(size, p))\n",
    "beta_true = rng.normal(size=p)\n",
    "probs = 1 / (1 + np.exp(-X @ beta_true))\n",
    "y = (rng.random(size=size) < probs).astype(int)\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=np.inf, max_iter=1000, fit_intercept=False)\n",
    "model.fit(X, y)  # type: ignore\n",
    "loss = log_loss(y, model.predict_proba(X)[:, 1])\n",
    "print(f\"model.coef_.round(2) = {model.coef_.squeeze().round(2)}\")\n",
    "print(f\"\\nloss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: my_log_loss(beta) = 1.0123999118804932\n",
      "i = 10: my_log_loss(beta) = 0.37464815378189087\n",
      "i = 20: my_log_loss(beta) = 0.3745999038219452\n",
      "i = 30: my_log_loss(beta) = 0.37459975481033325\n",
      "i = 40: my_log_loss(beta) = 0.37459975481033325\n",
      "\n",
      "beta.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n"
     ]
    }
   ],
   "source": [
    "def my_log_loss(beta):\n",
    "    return jnp.mean(y * jnp.log(1 + jnp.exp(-X @ beta)) + (1 - y) * jnp.log(1 + jnp.exp(X @ beta)))  # type: ignore\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=p))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(my_log_loss)(beta)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i = {i}: my_log_loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-sigmoid function is built into JAX. We can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: my_log_loss(beta) = 1.6029266119003296\n",
      "i = 10: my_log_loss(beta) = 0.37462809681892395\n",
      "i = 20: my_log_loss(beta) = 0.37459999322891235\n",
      "i = 30: my_log_loss(beta) = 0.37459975481033325\n",
      "i = 40: my_log_loss(beta) = 0.37459975481033325\n",
      "\n",
      "beta.round(2) = [-0.68 -0.46  0.4   0.25 -1.3   0.76 -0.43 -1.29  1.65  0.78]\n"
     ]
    }
   ],
   "source": [
    "from jax.nn import log_sigmoid\n",
    "\n",
    "\n",
    "def my_log_loss(beta):\n",
    "    return -jnp.mean(y * log_sigmoid(X @ beta) + (1 - y) * log_sigmoid(-X @ beta))\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=p))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(my_log_loss)(beta)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"i = {i}: my_log_loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### Softmax regression\n",
    "\n",
    "$$\n",
    "\\frac{p(v_j\\mid \\beta)}{p(v_0\\mid \\beta)} = e^{X\\beta_j},\\qquad 0 < j < n\\tag{$\\dag$}\n",
    "$$\n",
    "\n",
    "Solving\n",
    "$$\n",
    "\\frac1{p(v_0\\mid\\beta)} =\\sum_k \\frac{p(v_k\\mid \\beta)}{p(v_0\\mid\\beta)} = \\sum_k e^{X\\beta_k}\n",
    "$$\n",
    "for $p(v_0\\mid\\beta)$, we get\n",
    "$$\n",
    "p(v_0\\mid\\beta) = \\frac1{\\sum_j e^{X\\beta_j}}.\n",
    "$$\n",
    "\n",
    "Letting $\\beta_0=0$ (the zero vector), we have\n",
    "$$\n",
    "p(v_j\\mid\\beta)=\\frac{e^{X\\beta_j}}{\\sum_ke^{X\\beta_k}}\n",
    "$$\n",
    "for all $j$.\n",
    "\n",
    "Define the ***softmax function***:\n",
    "$$\n",
    "\\operatorname{softmax}:\\mathbb{R}^c\\longrightarrow \\mathbb{R}^c,\\qquad\n",
    "\\operatorname{softmax}(\\mu) = \\left(\n",
    "    \\frac{e^{\\mu_0}}{\\sum_k e^{\\mu_k}},\\ldots, \\frac{e^{\\mu_{c-1}}}{\\sum_k e^{\\mu_k}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "log-likelihood function:\n",
    "$$\n",
    "\\log p(y_i\\mid x_i, \\beta) = \\operatorname{sum}(y_i\\odot\\log\\operatorname{softmax}(x_i\\beta))\n",
    "$$\n",
    "\n",
    "Batched version:\n",
    "$$\n",
    "\\log p(Y\\mid X,\\beta) = \\operatorname{sum}(Y\\odot\\log\\operatorname{softmax}(X\\beta,\\, \\text{\\texttt{axis=1}}),\\, \\text{\\texttt{axis=1}})\n",
    "$$\n",
    "\n",
    "Conveniently, the log-softmax function is built into JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84 -0.16  1.12 -1.79]\n",
      " [ 0.3  -0.13 -1.56  1.39]\n",
      " [-0.66  0.03  1.44 -0.81]\n",
      " [-0.54  0.22  0.82 -0.5 ]\n",
      " [-0.1   1.41  0.41 -1.72]\n",
      " [-1.16  1.3   0.45 -0.59]\n",
      " [-0.02  0.4   0.88 -1.25]\n",
      " [-0.74  0.05 -0.02  0.72]\n",
      " [-1.54  0.34  0.73  0.47]\n",
      " [ 0.27  0.36 -0.61 -0.02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "size = 300\n",
    "p = 10\n",
    "c = 4\n",
    "rng = np.random.default_rng(42)\n",
    "X = rng.normal(size=(size, p))\n",
    "beta = rng.normal(size=(p, c))\n",
    "z = np.exp(X @ beta)\n",
    "probs = z / z.sum(axis=1, keepdims=True)\n",
    "y = np.argmax(rng.uniform(size=(size, 1)) < np.cumsum(probs, axis=1), axis=1)\n",
    "\n",
    "model = LogisticRegression(C=np.inf, max_iter=1000, fit_intercept=False, tol=1e-8)\n",
    "model.fit(X, y)  # type: ignore\n",
    "accuracy_score(y, model.predict(X))\n",
    "\n",
    "print(model.coef_.squeeze().round(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: loss(beta) = 3.732680559158325\n",
      "i = 2: loss(beta) = 0.5561644434928894\n",
      "i = 4: loss(beta) = 0.5440211892127991\n",
      "i = 6: loss(beta) = 0.542653501033783\n",
      "i = 8: loss(beta) = 0.5423337817192078\n",
      "i = 10: loss(beta) = 0.5422449111938477\n",
      "i = 12: loss(beta) = 0.5422179102897644\n",
      "i = 14: loss(beta) = 0.5422090291976929\n",
      "i = 16: loss(beta) = 0.542205810546875\n",
      "i = 18: loss(beta) = 0.5422044992446899\n",
      "i = 20: loss(beta) = 0.5422039031982422\n",
      "i = 22: loss(beta) = 0.5422036051750183\n",
      "i = 24: loss(beta) = 0.5422035455703735\n",
      "i = 26: loss(beta) = 0.542203426361084\n",
      "i = 28: loss(beta) = 0.542203426361084\n",
      "\n",
      "beta.round(2) = [[ 1.13  0.13  1.42 -1.5 ]\n",
      " [ 0.3  -0.13 -1.55  1.4 ]\n",
      " [-1.27 -0.59  0.83 -1.42]\n",
      " [-1.27 -0.51  0.1  -1.22]\n",
      " [-0.4   1.11  0.11 -2.02]\n",
      " [-1.85  0.61 -0.23 -1.28]\n",
      " [-0.41  0.01  0.49 -1.64]\n",
      " [-0.59  0.2   0.14  0.87]\n",
      " [-1.62  0.26  0.65  0.39]\n",
      " [-0.34 -0.26 -1.22 -0.64]]\n"
     ]
    }
   ],
   "source": [
    "from jax.nn import log_softmax\n",
    "\n",
    "Y = np.zeros((size, c))\n",
    "np.put_along_axis(arr=Y, indices=y.reshape(-1, 1), values=1, axis=1)\n",
    "\n",
    "\n",
    "def loss(beta):\n",
    "    return -jnp.sum(Y * log_softmax(X @ beta)) / size\n",
    "\n",
    "\n",
    "h = 10\n",
    "max_iter = 30\n",
    "beta = jnp.array(np.random.normal(size=(p, c)))  # type: ignore\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss)(beta)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"\\nbeta.round(2) = {np.array(beta).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74, -0.19,  0.95, -1.51],\n",
       "       [ 0.21, -0.05, -1.29,  1.13],\n",
       "       [-0.53, -0.04,  1.24, -0.67],\n",
       "       [-0.45,  0.16,  0.7 , -0.41],\n",
       "       [-0.04,  1.17,  0.28, -1.42],\n",
       "       [-0.96,  1.1 ,  0.35, -0.48],\n",
       "       [-0.02,  0.31,  0.74, -1.04],\n",
       "       [-0.63,  0.02, -0.01,  0.62],\n",
       "       [-1.3 ,  0.26,  0.6 ,  0.44],\n",
       "       [ 0.22,  0.33, -0.51, -0.03]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "model.coef_.round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "beta_skl = model.coef_.round(2).T\n",
    "loss(beta_skl)\n",
    "accuracy_score(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(X @ beta, axis=1)\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time using identifiable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(n: int, p: int, c: int, rng: int | np.random.Generator | None = None):\n",
    "    if c < 2:\n",
    "        raise ValueError(\"Number of classes c must be > 1.\")\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n, p))\n",
    "    beta = np.zeros((p, c))\n",
    "    beta[:, 1:] = rng.normal(size=(p, c - 1))\n",
    "    z = np.exp(X @ beta)\n",
    "    probs = z / z.sum(axis=1, keepdims=True)\n",
    "    y = np.argmax(\n",
    "        rng.uniform(size=(n, 1)) < np.cumsum(probs, axis=1), axis=1, keepdims=True\n",
    "    )\n",
    "    Y = np.zeros((n, c))\n",
    "    np.put_along_axis(arr=Y, indices=y, values=1, axis=1)\n",
    "    return X, Y, beta[:, 1:]\n",
    "\n",
    "\n",
    "def model(beta, X):\n",
    "    p = len(beta)\n",
    "    full_beta = jnp.hstack([jnp.zeros((p, 1)), beta])\n",
    "    logits = X @ full_beta\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(beta, X, Y):\n",
    "    logits = model(beta, X)\n",
    "    return -jnp.sum(Y * log_softmax(logits)) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = 0.7099\n",
      "i = 0: loss(beta) = 3.6220591068267822\n",
      "i = 5: loss(beta) = 0.7253149151802063\n",
      "i = 10: loss(beta) = 0.714459240436554\n",
      "i = 15: loss(beta) = 0.7138931155204773\n",
      "i = 20: loss(beta) = 0.7138315439224243\n",
      "i = 25: loss(beta) = 0.7138221263885498\n",
      "i = 30: loss(beta) = 0.7138205170631409\n",
      "i = 35: loss(beta) = 0.7138200998306274\n",
      "i = 40: loss(beta) = 0.7138200998306274\n",
      "i = 45: loss(beta) = 0.7138200998306274\n",
      "training loss = 0.7099\n",
      "max error = 1.93%\n"
     ]
    }
   ],
   "source": [
    "size = 10000000\n",
    "p = 10\n",
    "c = 4\n",
    "X, Y, _beta = data(size, p, c)\n",
    "print(\n",
    "    f\"true loss = {np.mean(\n",
    "        np.argmax(X @ np.hstack([np.zeros((p, 1)), _beta]), axis=1)\n",
    "        == np.argmax(Y, axis=1)\n",
    "    ):.4f}\"\n",
    ")\n",
    "\n",
    "h = 10\n",
    "max_iter = 50\n",
    "beta = jnp.array(np.random.normal(size=(p, c - 1)))\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, X, Y)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "\n",
    "print(\n",
    "    f\"training loss = {np.mean(\n",
    "        np.argmax(X @ np.hstack([np.zeros((p, 1)), beta]), axis=1)\n",
    "        == np.argmax(Y, axis=1)\n",
    "    ):.4f}\"\n",
    ")\n",
    "\n",
    "assert np.all(np.sign(beta) == np.sign(_beta))\n",
    "print(\n",
    "    f\"max error = {100*np.max((beta - _beta) / (0.5 * np.abs(beta + _beta))).item():.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson regression\n",
    "\n",
    "$$\n",
    "y_i\\sim \\operatorname{Poisson}(e^{x_i\\beta})\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y_i\\mid x_i,\\beta) = \\frac{e^{y_ix_i\\beta}e^{-e^{x_i\\beta}}}{y_i!}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y_i\\mid x_i,\\beta) &= \\log\\frac{e^{y_ix_i\\beta}e^{-e^{x_i\\beta}}}{y_i!}\\\\\n",
    "&= y_ix_i\\beta - e^{x_i\\beta} - \\log y_i!\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $y_i!$ is gigantic is $y_i$ is big, and we only want its logarithm anyways, it's best to avoid computing directly. Better to use the ***log-gamma function***:\n",
    "$$\n",
    "\\log y_i! = \\log\\Gamma(y_i + 1)\n",
    "$$\n",
    "\n",
    "Negative log-likelihood:\n",
    "$$\n",
    "\\ell(\\beta) = -\\frac1n\\operatorname{sum}(Y\\odot (X\\beta) - e^{X\\beta} - \\log \\Gamma(Y+1))\n",
    "$$\n",
    "\n",
    "We can actually drop the log-gamma term since it doesn't depend on $\\beta$.\n",
    "\n",
    "$$\n",
    "\\operatorname{loss} = -\\operatorname{mean}(Y\\odot (X\\beta) - e^{X\\beta})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(n: int, p: int, rng: int | np.random.Generator | None = None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n, p))\n",
    "    beta = rng.normal(size=p)\n",
    "    lam = np.exp(X @ beta)\n",
    "    y = rng.poisson(lam).astype(float)\n",
    "    return X, y, beta\n",
    "\n",
    "\n",
    "def loss(beta, X, y):\n",
    "    log_lam = X @ beta\n",
    "    return -jnp.mean(y * log_lam - jnp.exp(log_lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = -6.6776\n",
      "i = 0: loss(beta) = -0.44397902488708496\n",
      "i = 50: loss(beta) = -6.603349685668945\n",
      "i = 100: loss(beta) = -6.676112651824951\n",
      "i = 150: loss(beta) = -6.677542686462402\n",
      "i = 200: loss(beta) = -6.677570819854736\n",
      "i = 250: loss(beta) = -6.677570343017578\n",
      "i = 300: loss(beta) = -6.677570819854736\n",
      "i = 350: loss(beta) = -6.677570819854736\n",
      "i = 400: loss(beta) = -6.677570819854736\n",
      "i = 450: loss(beta) = -6.677570819854736\n",
      "training loss = -6.6776\n"
     ]
    }
   ],
   "source": [
    "size = 1000000\n",
    "p = 3\n",
    "X, y, _beta = data(size, p)\n",
    "print(f\"true loss = {loss(_beta, X, y):.4f}\")\n",
    "\n",
    "h = 0.01\n",
    "max_iter = 500\n",
    "beta = jnp.array(np.random.normal(size=p))\n",
    "# beta = jnp.zeros(p)\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, X, y)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"i = {i}: loss(beta) = {v}\")\n",
    "    beta -= h * g\n",
    "print(f\"training loss = {loss(beta, X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative binomial regression\n",
    "\n",
    "#### $(r, p)$-parametrization\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y\\mid n, p) &= \\binom{y + n - 1}{y}(1 - p)^yp^n\\\\\n",
    "&\\propto\\frac{\\Gamma(y + n)}{\\Gamma(n)}(1 - p)^yp^n\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y\\mid n, p) &\\propto  \\log\\Gamma(y + n) - \\log\\Gamma(n) + y\\log(1 - p) + n\\log p\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\\mu = \\mathbb{E}[y] = \\frac{n(1-p)}{p},\\qquad\n",
    "\\sigma^2=\\mathbb{V}[y] = \\frac{n(1-p)}{p^2} = \\mu + \\frac{\\mu^2}{n}\n",
    "$$\n",
    "\n",
    "Solve for $p$ in terms of $\\mu$ and $n$,\n",
    "$$\n",
    "p = \\frac{n}{\\mu + n},\n",
    "$$\n",
    "note that\n",
    "$$\n",
    "1 - p = \\frac{\\mu}{\\mu + n},\n",
    "$$\n",
    "and eliminate $p$ entirely:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y\\mid \\mu, n) &= \\binom{y + n - 1}{y}\n",
    "\\left(\\frac{\\mu}{\\mu + n}\\right)^y\n",
    "\\left(\\frac{n}{\\mu + n}\\right)^n\\\\\n",
    "&= \\frac{\\Gamma(y + n)}{y!\\Gamma(n)}\n",
    "\\left(\\frac{\\mu}{\\mu + n}\\right)^y\n",
    "\\left(\\frac{n}{\\mu + n}\\right)^n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(y\\mid \\mu, n) &= \\log\\Gamma(y + n) - \\log\\Gamma(n) - \\log y!\\\\\n",
    "&\\phantom{=}{} + y\\log\\mu - (y + n)\\log(\\mu + n) + n\\log n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mean = 8.04, distribution mean = 8.00\n",
      "sample variance = 24.28, distribution variance = 24.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax.scipy.stats as jstats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "p = 1 / 3\n",
    "size = 4\n",
    "size = 10_000\n",
    "\n",
    "# negative binomial sampling with numpy.random\n",
    "# no equivalent in jax.numpy.random\n",
    "y = rng.negative_binomial(size, p, size=size)\n",
    "counter = Counter(y)\n",
    "values = np.array(list(counter.keys()))\n",
    "counts = np.array(list(counter.values()))\n",
    "\n",
    "print(f\"sample mean = {y.mean():.2f}, distribution mean = {size*(1-p)/p:.2f}\")\n",
    "print(f\"sample variance = {y.var():.2f}, distribution variance = {size*(1-p)/p**2:.2f}\")\n",
    "\n",
    "# compute pmf-values using jax.scipy.stats.nbinom\n",
    "masses = jstats.nbinom.pmf(values, size, p)\n",
    "\n",
    "plt.vlines(values, 0, counts / size, colors=\"C0\", label=\"observed\")\n",
    "plt.vlines(values + 0.5, 0, masses, colors=\"C1\", label=\"predicted\")\n",
    "plt.title(f\"{size} draws from $\\\\operatorname{{NB}}(n={size}, p={p:.2f})$\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlim(-2, 32)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(beta, y):\n",
    "    n, p = beta\n",
    "    return -jstats.nbinom.logpmf(y, n, p).mean()\n",
    "\n",
    "\n",
    "def data(size: int, n: float | None = None, p: float | None = None, rng=None):\n",
    "    if n is None or p is None:\n",
    "        raise ValueError()\n",
    "    rng = np.random.default_rng(rng)\n",
    "    return rng.negative_binomial(n, p, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true loss = 2.9209\n",
      "i = 0: loss(beta) = 6.2822699546813965, n = 1.0000, p = 0.5000\n",
      "i = 200: loss(beta) = 2.9688873291015625, n = 2.1759, p = 0.2279\n",
      "i = 400: loss(beta) = 2.944277048110962, n = 2.5085, p = 0.2372\n",
      "i = 600: loss(beta) = 2.9359540939331055, n = 2.7379, p = 0.2534\n",
      "i = 800: loss(beta) = 2.931231737136841, n = 2.9109, p = 0.2652\n",
      "i = 1000: loss(beta) = 2.9282848834991455, n = 3.0478, p = 0.2743\n",
      "i = 1200: loss(beta) = 2.92632794380188, n = 3.1593, p = 0.2815\n",
      "i = 1400: loss(beta) = 2.9249696731567383, n = 3.2522, p = 0.2874\n",
      "i = 1600: loss(beta) = 2.9239964485168457, n = 3.3309, p = 0.2923\n",
      "i = 1800: loss(beta) = 2.9232850074768066, n = 3.3982, p = 0.2965\n",
      "i = 2000: loss(beta) = 2.9227511882781982, n = 3.4565, p = 0.3000\n",
      "i = 2200: loss(beta) = 2.9223477840423584, n = 3.5072, p = 0.3031\n",
      "i = 2400: loss(beta) = 2.922037363052368, n = 3.5517, p = 0.3058\n",
      "i = 2600: loss(beta) = 2.9217963218688965, n = 3.5910, p = 0.3081\n",
      "i = 2800: loss(beta) = 2.9216060638427734, n = 3.6257, p = 0.3102\n",
      "i = 3000: loss(beta) = 2.9214558601379395, n = 3.6566, p = 0.3120\n",
      "i = 3200: loss(beta) = 2.921337127685547, n = 3.6841, p = 0.3136\n",
      "i = 3400: loss(beta) = 2.9212424755096436, n = 3.7087, p = 0.3150\n",
      "i = 3600: loss(beta) = 2.921165943145752, n = 3.7308, p = 0.3163\n",
      "i = 3800: loss(beta) = 2.9211044311523438, n = 3.7506, p = 0.3175\n",
      "i = 4000: loss(beta) = 2.9210526943206787, n = 3.7684, p = 0.3185\n",
      "i = 4200: loss(beta) = 2.921013832092285, n = 3.7845, p = 0.3194\n",
      "i = 4400: loss(beta) = 2.92098069190979, n = 3.7991, p = 0.3203\n",
      "i = 4600: loss(beta) = 2.920954704284668, n = 3.8122, p = 0.3210\n",
      "i = 4800: loss(beta) = 2.9209303855895996, n = 3.8241, p = 0.3217\n",
      "i = 5000: loss(beta) = 2.9209144115448, n = 3.8348, p = 0.3223\n",
      "i = 5200: loss(beta) = 2.9208991527557373, n = 3.8446, p = 0.3229\n",
      "i = 5400: loss(beta) = 2.9208855628967285, n = 3.8535, p = 0.3234\n",
      "i = 5600: loss(beta) = 2.9208736419677734, n = 3.8615, p = 0.3238\n",
      "i = 5800: loss(beta) = 2.9208662509918213, n = 3.8688, p = 0.3242\n",
      "i = 6000: loss(beta) = 2.920860528945923, n = 3.8755, p = 0.3246\n",
      "i = 6200: loss(beta) = 2.92085337638855, n = 3.8815, p = 0.3250\n",
      "i = 6400: loss(beta) = 2.920849561691284, n = 3.8870, p = 0.3253\n",
      "i = 6600: loss(beta) = 2.9208452701568604, n = 3.8921, p = 0.3255\n",
      "i = 6800: loss(beta) = 2.9208436012268066, n = 3.8966, p = 0.3258\n",
      "i = 7000: loss(beta) = 2.9208412170410156, n = 3.9008, p = 0.3260\n",
      "i = 7200: loss(beta) = 2.920836925506592, n = 3.9046, p = 0.3263\n",
      "i = 7400: loss(beta) = 2.920834541320801, n = 3.9081, p = 0.3264\n",
      "i = 7600: loss(beta) = 2.9208333492279053, n = 3.9113, p = 0.3266\n",
      "i = 7800: loss(beta) = 2.920833110809326, n = 3.9142, p = 0.3268\n",
      "i = 8000: loss(beta) = 2.9208312034606934, n = 3.9168, p = 0.3269\n",
      "i = 8200: loss(beta) = 2.920830488204956, n = 3.9192, p = 0.3271\n",
      "i = 8400: loss(beta) = 2.920828342437744, n = 3.9214, p = 0.3272\n",
      "i = 8600: loss(beta) = 2.9208309650421143, n = 3.9234, p = 0.3273\n",
      "i = 8800: loss(beta) = 2.920828342437744, n = 3.9253, p = 0.3274\n",
      "i = 9000: loss(beta) = 2.920830011367798, n = 3.9269, p = 0.3275\n",
      "i = 9200: loss(beta) = 2.9208264350891113, n = 3.9285, p = 0.3276\n",
      "i = 9400: loss(beta) = 2.920828104019165, n = 3.9299, p = 0.3277\n",
      "i = 9600: loss(beta) = 2.9208261966705322, n = 3.9311, p = 0.3277\n",
      "i = 9800: loss(beta) = 2.920828342437744, n = 3.9323, p = 0.3278\n",
      "training loss = 2.9208\n",
      "n = 3.9334, p = 0.3279\n"
     ]
    }
   ],
   "source": [
    "y = data(10000, n=4, p=1 / 3)\n",
    "_beta = jnp.array([4, 1 / 3])\n",
    "print(f\"true loss = {loss(_beta, y):.4f}\")\n",
    "\n",
    "beta = jnp.array([1.0, 0.5])\n",
    "h = 0.032\n",
    "max_iter = 10000\n",
    "for i in range(max_iter):\n",
    "    v, g = value_and_grad(loss, 0)(beta, y)\n",
    "    if i % 200 == 0:\n",
    "        print(\n",
    "            f\"i = {i}: loss(beta) = {v}, n = {beta[0].item():.4f}, p = {beta[1].item():.4f}\"\n",
    "        )\n",
    "    beta -= h * g\n",
    "print(f\"training loss = {loss(beta, y):.4f}\")\n",
    "\n",
    "size = beta[0].item()\n",
    "p = beta[1].item()\n",
    "print(f\"n = {size:.4f}, p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom n = 3.8758, mom p = 0.3246\n"
     ]
    }
   ],
   "source": [
    "mu = y.mean()\n",
    "sigma = y.std()\n",
    "\n",
    "mom_n = mu**2 / (sigma**2 - mu)\n",
    "mom_p = mom_n / (mu + mom_n)\n",
    "print(f\"mom n = {mom_n:.4f}, mom p = {mom_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(\n",
    "    n_examples: int,\n",
    "    n_features: int,\n",
    "    n: float,\n",
    "    rng: int | np.random.Generator | None = None,\n",
    "):\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    rng = np.random.default_rng(rng)\n",
    "    X = rng.normal(size=(n_examples, n_features))\n",
    "    beta = rng.normal(size=n_features)\n",
    "    mu = np.exp(X @ beta)\n",
    "    p = n / (mu + n)\n",
    "    y = rng.negative_binomial(n, p)\n",
    "    return X, y, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, beta = data(300, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  3,  1,  1,  4,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0, 18,  3,\n",
       "        5,  0,  0,  0,  1,  0,  0,  7,  1, 10,  0,  1,  1,  0, 31,  0,  0,\n",
       "        0,  0, 16,  4,  0,  0,  6,  0,  4,  1,  0,  0,  4,  0,  3,  0,  9,\n",
       "        1,  2,  1,  4,  4,  9,  2,  0,  1,  4,  0,  3,  2,  3,  1, 19,  1,\n",
       "        0,  1,  2,  0,  0, 12,  0,  0,  2,  0,  0,  0,  0,  0,  3,  0,  0,\n",
       "        3,  4,  1,  0,  0,  0,  0,  3,  6,  2,  2,  0,  1,  0,  2,  9,  2,\n",
       "        0,  2,  2,  0,  0,  1,  0,  8,  1,  4,  2,  3,  0,  0,  1,  0,  2,\n",
       "        1,  4, 24,  0,  1,  2,  4,  3,  0,  0,  0,  0,  0,  1,  5,  3,  0,\n",
       "       12,  3,  3,  1,  7,  0,  0,  0, 10,  1,  0,  1,  2,  0, 23,  1,  0,\n",
       "        0,  0,  5,  1,  3,  1,  2,  0,  1, 22,  0,  1,  0,  0, 68, 11,  1,\n",
       "        1,  6,  0,  0,  0,  0,  0,  0,  1,  6,  6,  1,  2,  2,  0,  6,  0,\n",
       "        0, 10,  0,  0,  6,  0,  1,  0, 29,  7,  0,  3,  0,  2, 13,  1,  1,\n",
       "        0,  1,  2,  2,  0,  0,  1,  1,  1,  0,  3,  0,  0,  0,  6,  0,  9,\n",
       "        2,  1,  0,  0,  1,  2,  0,  0,  3, 40,  4,  5,  3,  1,  0,  1,  2,\n",
       "        3,  0,  6,  2,  6,  0,  5,  1,  3,  7,  2,  1,  6,  0,  1,  0,  4,\n",
       "        1,  1,  2,  0,  0, 13,  0,  1,  3,  0,  1,  0,  0,  0,  2,  2,  3,\n",
       "        0,  0,  3,  3,  1,  0,  4,  3,  0,  1,  0,  0,  0,  1,  0,  0,  0,\n",
       "        1,  1,  2,  0,  1,  1,  0,  0,  1,  0,  0])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
